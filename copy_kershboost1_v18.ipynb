{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: kershnet1_v.ipynb\n",
    "# Created: March 2019\n",
    "# Last Updated: April 2019\n",
    "# POC: mlwinterrose@gmail.com\n",
    "# Description: Predict Clayton Kershaw's next pitch based on \n",
    "#              11 years of historical Kershaw data, which \n",
    "#              includes game state, count in the at-bat, . . . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Y. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Data prep for ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ML algos\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# ML models\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# ML model evaluation\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.metrics import recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "# Model interpretation\n",
    "import shap\n",
    "shap.initjs() # \"print the JS viz code to the notebook\"\n",
    "\n",
    "# Misc\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "import time ## directory and file labels\n",
    "import os                                                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Y. Data Ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/df_kershaw_statcast_2008thru2018.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "#print(df.shape)\n",
    "#print(df.columns)\n",
    "#df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Y. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(df_in, feature_list_in):\n",
    "    \"\"\"\n",
    "        Select a subset of features from which to build a \n",
    "        predictive model.\n",
    "    \"\"\"\n",
    "    df_out = df_in[feature_list_in]\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def remove_nans(df_in, col_list):\n",
    "    \"\"\"\n",
    "        Deletes rows in df_in that contain NaNs in the columns in col_list\n",
    "    \"\"\"\n",
    "    df_out = df_in.dropna(subset=[col_list])\n",
    "    return df_out\n",
    "\n",
    "def select_pitch_type_target(df_in, target_pitch_in):\n",
    "    \"\"\"\n",
    "        Tags the pitch type given in 'target_pitch_in' as '1', \n",
    "        and all other pitch types as '0', in preparation\n",
    "        for binary classification step\n",
    "    \"\"\"\n",
    "    ### turn pitch_type into binary (to start with). Just: is FF or is not:\n",
    "    df_out = df_in.copy()\n",
    "    df_out['pitch_bin'] = 0\n",
    "    # df_out.loc[df_out.pitch_type == 'FF', 'FF_bin'] = 1\n",
    "    df_out.loc[df_out.pitch_type == target_pitch_in, 'pitch_bin'] = 1\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for data selection\n",
    "feature_list = ['pitch_type', 'balls', 'strikes', 'game_year', \n",
    "                'batter', 'pitch_number', 'at_bat_number', \n",
    "                'home_score','away_score', 'if_fielding_alignment',\n",
    "                'of_fielding_alignment', 'on_3b', 'on_2b', 'on_1b', \n",
    "                'outs_when_up' \n",
    "               ]\n",
    "remove_nans_from_cols = 'pitch_type'\n",
    "target_pitch_class = 'FF' #'CH' #'CU' #'SL' #'FF' #'SL' #'CU' #'FF' #'CU' #'FF'\n",
    "test_size = 0.3\n",
    "nan_strategy = 'fill_zero'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = select_features(df, feature_list)\n",
    "df = remove_nans(df, remove_nans_from_cols) ## a hack . . . \n",
    "df = select_pitch_type_target(df, target_pitch_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Y. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pitch_bin'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pitch_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### viz what type of pitch kersh throws in what type of count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df[['balls','strikes','pitch_type']], hue='pitch_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### new NB, viz or EDA kersh data . . . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Y. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Y. Prepare data for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## num_train0, num_train1, frac_class1 = calc_class_imbal(y_train)\n",
    "\n",
    "def calc_class_imbal(y_in):\n",
    "    \"\"\"\n",
    "    Insert Function Description\n",
    "    \"\"\"\n",
    "    df_ytrain = pd.DataFrame(y_in)\n",
    "    df_ytrain.columns = ['y']\n",
    "    df_y_0 = df_ytrain[df_ytrain['y'] == 0]\n",
    "    df_y_1 = df_ytrain[df_ytrain['y'] == 1]\n",
    "    num_train0_out = df_y_0.shape[0]\n",
    "    num_train1_out = df_y_1.shape[0]\n",
    "    frac_0 = num_train0_out / num_train1_out\n",
    "    print(\" \")\n",
    "    print('num_train0_out: ', num_train0_out)\n",
    "    print('num_train1_out: ', num_train1_out)\n",
    "    print('frac_0: (num_train0_out / num_train1_out): ', frac_0)\n",
    "    return frac_0\n",
    "\n",
    "def standardize_data(X_train_in, X_test_in):\n",
    "    \"\"\"\n",
    "    Insert Function Description\n",
    "    \"\"\"\n",
    "    sc = StandardScaler()\n",
    "    sc.fit(X_train_in)\n",
    "    X_train_std_out = sc.transform(X_train_in)\n",
    "    X_test_std_out = sc.transform(X_test_in)\n",
    "    return X_train_std_out, X_test_std_out\n",
    "\n",
    "def pca_dim_red(X_train_in, X_test_in):\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    X_train_pca_out = pca.fit_transform(X_train_in)\n",
    "    pca.fit_transform(X_train_in)\n",
    "    X_test_pca_out = pca.transform(X_test_in)\n",
    "    \n",
    "    return X_train_pca_out, X_test_pca_out\n",
    "\n",
    "def create_X_y_dfs(df_in): \n",
    "    \"\"\"\n",
    "        Hard coded for this specific dataset (generalize later)\n",
    "    \"\"\"\n",
    "    df_y_out = pd.DataFrame(df_in['pitch_bin'])\n",
    "    df_X_out = df_in.drop(columns=['pitch_type', 'pitch_bin'], axis=1)\n",
    "    return df_X_out, df_y_out\n",
    "\n",
    "def create_numeric_and_onehot_features(df_in):\n",
    "    \"\"\"\n",
    "    Insert Function Description\n",
    "    \"\"\"    \n",
    "    df_out = pd.get_dummies(df_in)\n",
    "    return df_out\n",
    "\n",
    "def handle_nans(df_in, nan_strategy_in):\n",
    "    \"\"\"\n",
    "    Insert Function Description\n",
    "    \"\"\"   \n",
    "    df_out = df_in\n",
    "    if (nan_strategy_in == 'fill_zero'):\n",
    "        df_out = df_in.fillna(0)\n",
    "    return df_out\n",
    "\n",
    "def create_ml_data_splits(df_X_in, df_y_in, test_size_in):\n",
    "    \"\"\"\n",
    "    Insert Function Description\n",
    "    \"\"\"\n",
    "    X = df_X_in.values\n",
    "    y = df_y_in.values\n",
    "    X_train_out, X_test_out, y_train_out, y_test_out = train_test_split(X, y, test_size=test_size, \n",
    "                                                                        stratify=y) # random_state=1\n",
    "    return X_train_out, X_test_out, y_train_out, y_test_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.3\n",
    "nan_strategy = 'fill_zero'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_X, df_y = create_X_y_dfs(df)\n",
    "df_X = create_numeric_and_onehot_features(df_X)\n",
    "df_X = handle_nans(df_X, nan_strategy)\n",
    "\n",
    "X_train, X_test, y_train, y_test = create_ml_data_splits(df_X, df_y, test_size) \n",
    "frac_class0 = calc_class_imbal(y_train)\n",
    "\n",
    "X_train_std, X_test_std = standardize_data(X_train, X_test)\n",
    "X_train_pca, X_test_pca = pca_dim_red(X_train_std, X_test_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Y. ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_ml_model(label, y_test_in, y_pred_in):\n",
    "    \"\"\"\n",
    "    Evaluate maching learning (ML) model, binary classification\n",
    "    \"\"\"\n",
    "    print(label)\n",
    "    print(\" \")\n",
    "    \n",
    "    score_accuracy = accuracy_score(y_test_in, y_pred_in)\n",
    "    score_precision = precision_score(y_test_in, y_pred_in)\n",
    "    score_recall = recall_score(y_test_in, y_pred_in)\n",
    "    score_f1 = f1_score(y_test_in, y_pred_in)\n",
    "    score_roc_auc = roc_auc_score(y_test_in, y_pred_in)\n",
    "    score_confmat = confusion_matrix(y_test_in, y_pred_in)\n",
    "    \n",
    "    print('Accuracy: ', score_accuracy)\n",
    "    print('Precision: ', score_precision)\n",
    "    print('Recall: ', score_recall)\n",
    "    print('F1: ', score_f1)\n",
    "    print('ROC_AUC:', score_roc_auc)\n",
    "    print(score_confmat)\n",
    "    viz_confusion_matrix(score_confmat)\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_confusion_matrix(confmat):\n",
    "    fig, ax = plt.subplots(figsize=(2.5, 2.5))\n",
    "    ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n",
    "    for i in range(confmat.shape[0]):\n",
    "        for j in range(confmat.shape[1]):\n",
    "            ax.text(x=j, y=i, s=confmat[i, j], va='center', ha='center')\n",
    "\n",
    "    plt.xlabel('predicted label')\n",
    "    plt.ylabel('true label')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('./figures/confusion_matrix.png', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viz_roc(X_train, y_train, X_test, y_test, clf_tree_base)\n",
    "\n",
    "def viz_roc(X_train, y_train, X_test, y_test, clf):\n",
    "    \n",
    "    \"\"\"\n",
    "    This code is not (apparently) out of date. Update it to modern later . . . \n",
    "    \n",
    "    . . . yup, sebastian book 2E has updated code on roc viz . . . dig in on this later\n",
    "    \n",
    "    . . . so later, do get into skopt and others, definitely . . . and also let's get \n",
    "    a roc code up (and maybe see if can do a new decision region code? . . . ) (or can\n",
    "    just note that new book code (updated) is there and move into other things (skopt) later)\n",
    "    (now: eat and routine and sleep. fun w/ sabrm reads . . . ).\n",
    "    \"\"\"\n",
    "    \n",
    "    from sklearn.metrics import roc_curve, auc ## move up when done\n",
    "    from scipy import interp ## move up when done\n",
    "    #from sklearn.cross_validation import StratifiedKFold\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    \n",
    "    #cv = StratifiedKFold(y_train, n_folds=5, random_state=1)\n",
    "    #cv = StratifiedKFold(n_splits=5)\n",
    "        \n",
    "    fig = plt.figure(figsize=(7, 5))\n",
    "    mean_tpr = 0.0\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    all_tpr = []\n",
    "    \n",
    "    for i, (train, test) in enumerate(cv):\n",
    "        #pdb.set_trace()\n",
    "        #probas = tree.fit(X_train[train],\n",
    "        #                     y_train[train]).predict_proba(X_train[test])\n",
    "            \n",
    "        probas = tree.fit(X_train[train], list(y_train[train])).predict_proba(X_train[test])\n",
    "\n",
    "            \n",
    "        fpr, tpr, thresholds = roc_curve(list(y_train[test]),\n",
    "                                         probas[:, 1],\n",
    "                                         pos_label=1)\n",
    "        #probas = pipe_lr.fit(X_train2[train], y_train[train]).predict_proba(X_train2[test]) fpr, tpr, thresholds = roc_curve(y_train[test], probas[:, 1],pos_label=1)\n",
    "        #print(i)\n",
    "            \n",
    "        mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "        mean_tpr[0] = 0.0\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        #plt.plot(fpr, \n",
    "        #         tpr, \n",
    "        #         lw=1, \n",
    "        #         label='ROC fold %d (area = %0.2f)' \n",
    "        #                % (i+1, roc_auc))\n",
    "        \n",
    "    plt.plot([0, 1], \n",
    "             [0, 1], \n",
    "             linestyle='--', \n",
    "             color=(0.6, 0.6, 0.6), \n",
    "             label='Random Guessing')\n",
    "        \n",
    "    mean_tpr /= len(cv)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    plt.plot(mean_fpr, mean_tpr, 'k--',\n",
    "             label='Detector Performance (AUC = %0.2f)' % mean_auc, lw=2, color='red')\n",
    "        \n",
    "    #  label='Frequency Representation (AUC = %0.2f)' % mean_auc, lw=2, color='red')\n",
    "        \n",
    "    #label='mean ROC (area = %0.2f)' % mean_auc, lw=2)\n",
    "    #plt.plot([0, 0, 1], \n",
    "    # [0, 1, 1], \n",
    "    # lw=2, \n",
    "    # linestyle=':', \n",
    "    # color='black', \n",
    "    # label='perfect performance')\n",
    "        \n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    #plt.ylabel('true positive rate')\n",
    "    plt.ylabel('Attack Detection Rate')\n",
    "    #plt.title('Receiver Operator Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('./figures/roc.png', dpi=300)\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_2d_dec_bnd(X_in, y_in, clf_in, test_idx=None, resolution=0.02):\n",
    "    \"\"\"\n",
    "    Purpose is just to give a viz sense of what algo learning.\n",
    "    Working in 2D, hparams not optimized to 2D, etc. \n",
    "    Just gives a sense of it . . . \n",
    "    \n",
    "    (this function not working without crashing yet . . . return to . . . )\n",
    "    \"\"\"\n",
    "    \n",
    "    X = X_in\n",
    "    y = y_in\n",
    "    \n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    #X_test_pca = pca.transform(X_test_in)\n",
    "    \n",
    "    clf_in.fit(X_pca, y)    \n",
    "    \n",
    "    ### (from sebastian book, think . . . )\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    \n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    \n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution))\n",
    "    #Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = clf_in.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "    # plot class samples\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],alpha=0.8, c=cmap(idx),marker=markers[idx], label=cl)\n",
    "    # highlight test samples\n",
    "    if test_idx:\n",
    "        #print(\"I'm in!!!\")\n",
    "        X_test, y_test = X[test_idx, :], y[test_idx]\n",
    "        plt.scatter(X_test[:, 0], X_test[:, 1], c='', alpha=1.0, linewidths=1, marker='o',s=55, label='test set')\n",
    "    ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_regions(X, y, classifier,test_idx=None, resolution=0.02):\n",
    "    # setup marker generator and color map\n",
    "    \n",
    "    from matplotlib.colors import ListedColormap\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    \n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "    # plot class samples\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],alpha=0.8, c=cmap(idx),marker=markers[idx], label=cl)\n",
    "    # highlight test samples\n",
    "    if test_idx:\n",
    "        #print(\"I'm in!!!\")\n",
    "        X_test, y_test = X[test_idx, :], y[test_idx]\n",
    "        plt.scatter(X_test[:, 0], X_test[:, 1], c='', alpha=1.0, linewidths=1, marker='o',s=55, label='test set')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Y. Tree Hyper Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Baseline')\n",
    "\n",
    "clf_tree_base = []\n",
    "clf_tree_base = tree.DecisionTreeClassifier()\n",
    "\n",
    "print(clf_tree_base)\n",
    "\n",
    "# viz_2d_dec_bnd(X_train, y_train, clf_tree_base) # not working yet. return to.\n",
    "# viz_roc(X_train, y_train, X_test, y_test, clf_tree_base) # ok. my act roc_viz code is too out of date . . . \n",
    "                                                           # start over on it later . . . modernize\n",
    "clf_tree_base.fit(X_train, y_train)\n",
    "y_pred_tree_base = []\n",
    "y_pred_tree_base = clf_tree_base.predict(X_test)\n",
    "eval_ml_model('Classification Tree, Params: Default, Pitch: ' + target_pitch_class, y_test, y_pred_tree_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter_in = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Hyper Tuned !')\n",
    "\n",
    "clf = []\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "hparams = {}\n",
    "hparams['max_depth'] = np.linspace(1,32,32, endpoint=True) # alone: auc=0.6536 (best just on own)\n",
    "hparams['min_samples_split'] = np.linspace(0.1, 1.0, 10, endpoint=True)\n",
    "#hparams['min_samples_leaf'] = np.linspace(0.1, 0.5, 5, endpoint=True)\n",
    "#hparams['max_features'] = list(range(1,X_train.shape[1]))\n",
    "\n",
    "clf_best = GridSearchCV(clf, hparams, cv=5, verbose=0)\n",
    "\n",
    "clf_best = RandomizedSearchCV(clf, hparams,\n",
    "                              n_iter=n_iter_in, cv=5, verbose=0) # n_iter=100 #random_state=1\n",
    "\n",
    "clf_best.fit(X_train, y_train)\n",
    "\n",
    "# print(\"scorer_ : \", clf_best.scorer_) # scorer_ :  <function _passthrough_scorer at 0x1a24d7d378>\n",
    "print(\"best_score_ : \", clf_best.best_score_)\n",
    "print(\"best_params_ :\", clf_best.best_params_)\n",
    "\n",
    "y_pred1 = clf_best.predict(X_test)\n",
    "\n",
    "eval_ml_model('Tree, Params: Grid1, Pitch: ' + target_pitch_class, y_test, y_pred1)\n",
    "\n",
    "df_clf_best = pd.DataFrame(clf_best.cv_results_) ## awesome. this is perfect !!!\n",
    "\n",
    "#df_clf_best.to_csv('hparams_decTree_randomCV_budg' + \n",
    "#                   str(n_iter_in) + '_' + str(round(time.time())) + \n",
    "#                   '.csv')\n",
    "\n",
    "# probably belongs in it's own function .  . \n",
    "timestamp_out = str(round(time.time()))\n",
    "directory_out_name = 'data_out_dectree_' + timestamp_out\n",
    "os.mkdir(directory_out_name)\n",
    "\n",
    "#df_best_params = pd.DataFrame({'hparams':hparams,\n",
    "#                               'n_iter':n_iter_in,\n",
    "#                               'best_score': clf_best.best_score_,\n",
    "#                               'best_params': clf_best.best_params_\n",
    "#                              })\n",
    "\n",
    "#df_best_params.to_csv('/' + directory_out_name + '/' +\n",
    "#                      'best_params_decTree_randomCV_budg' + \n",
    "#                      str(n_iter_in) + '_' + str(round(time.time())) + \n",
    "#                      '.csv')\n",
    "\n",
    "df_best_params.to_csv(directory_out_name + '/' +\n",
    "                      'best_params_decTree_randomCV_budg' + \n",
    "                      str(n_iter_in) + '_' + str(round(time.time())) + \n",
    "                      '.csv')\n",
    "\n",
    "df_best_params = pd.DataFrame({'hparams':hparams,\n",
    "                               'n_iter':n_iter_in,\n",
    "                               'best_score': clf_best.best_score_,\n",
    "                               'best_params': clf_best.best_params_\n",
    "                              })\n",
    "\n",
    "df_clf_best.to_csv(directory_out_name + '/' +\n",
    "                   'hparams_decTree_randomCV_budg' + \n",
    "                   str(n_iter_in) + '_' + str(round(time.time())) + \n",
    "                   '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(round(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out csv creation to track hyper_param tuning . . . \n",
    "\n",
    "# useful: https://stackoverflow.com/questions/34274598/does-gridsearchcv-store-all-the-scores-for-all-parameter-combinations\n",
    "\n",
    "#clf_best.cv_results_ #nice\n",
    "\n",
    "#clf_best.cv_results_['mean_test_score'] #nice\n",
    "\n",
    "#clf_best.cv_results_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clf_best = pd.DataFrame(clf_best.cv_results_) ## awesome. this is perfect !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_clf_best.shape)\n",
    "print(df_clf_best.columns)\n",
    "df_clf_best.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clf_best.to_csv('df_clf_best_lookAt1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Y. XGB Hyper Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier()\n",
    "    \n",
    "hparams = {}\n",
    "#hparams['num_estimators'] = [50, 100, 150, 200]\n",
    "hparams['num_estimators'] = [50, 100]\n",
    "#hparams['max_depth'] = [4,6,8,10]\n",
    "hparams['max_depth'] = [4,8]\n",
    "    \n",
    "#clf.fit(X_train_in, y_train_in)\n",
    "    \n",
    "clf_best = GridSearchCV(clf, hparams, cv=5, verbose=0)\n",
    "clf_best.fit(X_train, y_train)\n",
    "\n",
    "print(\"scorer_ : \", clf_best.scorer_)\n",
    "print(\"best_score_ : \", clf_best.best_score_)\n",
    "print(\"best_params_ :\", clf_best.best_params_)\n",
    "\n",
    "y_pred_xgb_grid1 = clf_best.predict(X_test)\n",
    "    \n",
    "#print('Target Pitch: ', target_pitch_in)\n",
    "\n",
    "eval_ml_model('XGBoost, Params: Grid1, Pitch: ' + target_pitch_class, y_test, y_pred_xgb_grid1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see. that is a reasonable amount of time . . . why is SKOPT SO slow ?\n",
    "\n",
    "(run it and go eat . . . see if skopt ever stops . . . )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clf_best_xgb_grid1 = pd.DataFrame(clf_best.cv_results_)\n",
    "#clf_best\n",
    "#clf_best.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clf_best_xgb_grid1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier()\n",
    "    \n",
    "hparams = {}\n",
    "#hparams['num_estimators'] = [50, 100, 150, 200]\n",
    "hparams['num_estimators'] = [50, 100]\n",
    "#hparams['max_depth'] = [4,6,8,10]\n",
    "hparams['max_depth'] = [4,8]\n",
    "\n",
    "    \n",
    "#clf.fit(X_train_in, y_train_in)\n",
    "    \n",
    "#clf_best = GridSearchCV(clf, hparams, cv=5, verbose=0)\n",
    "clf_best = RandomizedSearchCV(clf, hparams, random_state=1,\n",
    "                              n_iter=10, cv=5, verbose=0) # n_iter=100\n",
    "\n",
    "clf_best.fit(X_train, y_train)\n",
    "\n",
    "print(\"best_score_ : \", clf_best.best_score_)\n",
    "print(\"best_params_ :\", clf_best.best_params_)\n",
    "\n",
    "    \n",
    "y_pred_xgb_rand1 = clf_best.predict(X_test)\n",
    "    \n",
    "#print('Target Pitch: ', target_pitch_in)\n",
    "\n",
    "eval_ml_model('XGBoost, Params: Rand1, Pitch: ' + target_pitch_class, y_test, y_pred_xgb_rand1)\n",
    "\n",
    "df_clf_best_xgb_rand1 = pd.DataFrame(clf_best.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_clf_best_xgb_rand1.shape)\n",
    "df_clf_best_xgb_rand1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier()\n",
    "    \n",
    "hparams = {}\n",
    "#hparams['num_estimators'] = [50, 100, 150, 200]\n",
    "\n",
    "#hparams['num_estimators'] = [50, 100]\n",
    "#hparams['num_estimators'] = [50, 51]\n",
    "hparams['num_estimators'] = [50]\n",
    "\n",
    "#hparams['max_depth'] = [4,6,8,10]\n",
    "#hparams['max_depth'] = [4,8]\n",
    "\n",
    "#hparams['max_depth'] = [4,5]\n",
    "hparams['max_depth'] = [4]\n",
    "    \n",
    "    \n",
    "#clf.fit(X_train_in, y_train_in)\n",
    "    \n",
    "#clf_best = GridSearchCV(clf, hparams, cv=5, verbose=0)\n",
    "clf_best = BayesSearchCV(clf, hparams, cv=5, verbose=0) ## will it literally work exactly same if want it to?\n",
    "\n",
    "clf_best.fit(X_train, y_train)\n",
    "\n",
    "print(\"best_score_ : \", clf_best.best_score_)\n",
    "print(\"best_params_ :\", clf_best.best_params_)\n",
    "    \n",
    "y_pred_xgb_bayes1 = clf_best.predict(X_test)\n",
    "    \n",
    "#print('Target Pitch: ', target_pitch_in)\n",
    "\n",
    "eval_ml_model('XGBoost, Params: BayesSearchCV1, Pitch: ' + target_pitch_class, y_test, y_pred_xgb_bayes1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5/20/2019, 2:15am: ok. this one is taking forever . . . study how param supposed to be specified . . . and get them in there right . . . and then und how to get the tuning csv want out (same as sklearn or diff? . . . ).\n",
    "\n",
    "#### ok. take this up post thing tomm (and then other sh, tons.). Now, go to sleep. asap. GO. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_clf_best_xgb_rand1 = pd.DataFrame(clf_best.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Y. Ok. Here we go: 5/25/2019: let's fix parameter input syntax\n",
    "\n",
    "(using example from: http://localhost:8889/notebooks/Desktop/scikit-optimize-master/examples/sklearn-gridsearchcv-replacement.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier()\n",
    "    \n",
    "hparams = {}\n",
    "#hparams['num_estimators'] = [50, 100, 150, 200]\n",
    "\n",
    "#hparams['num_estimators'] = [50, 100]\n",
    "#hparams['num_estimators'] = [50, 51]\n",
    "#hparams['num_estimators'] = [50]\n",
    "\n",
    "hparams['num_estimators'] = Integer(50,51)\n",
    "\n",
    "#hparams['max_depth'] = [4,6,8,10]\n",
    "#hparams['max_depth'] = [4,8]\n",
    "\n",
    "#hparams['max_depth'] = [4,5]\n",
    "#hparams['max_depth'] = [4]\n",
    "hparams['max_depth'] = Integer(4,5)\n",
    "    \n",
    "    \n",
    "#clf.fit(X_train_in, y_train_in)\n",
    "    \n",
    "#clf_best = GridSearchCV(clf, hparams, cv=5, verbose=0)\n",
    "clf_best = BayesSearchCV(clf, hparams, cv=5, verbose=0) ## will it literally work exactly same if want it to?\n",
    "\n",
    "clf_best.fit(X_train, y_train)\n",
    "\n",
    "print(\"best_score_ : \", clf_best.best_score_)\n",
    "print(\"best_params_ :\", clf_best.best_params_)\n",
    "    \n",
    "y_pred_xgb_bayes1 = clf_best.predict(X_test)\n",
    "    \n",
    "#print('Target Pitch: ', target_pitch_in)\n",
    "\n",
    "eval_ml_model('XGBoost, Params: BayesSearchCV1, Pitch: ' + target_pitch_class, y_test, y_pred_xgb_bayes1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Y. Shap model interpretations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Classic feature attributions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(xgb.plot_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(figsize=(5,8))\n",
    "xgb.plot_importance(clf_xgb_base, height=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (work on getting real feature names in there .  .  . can I pass the df into xgboost . . . expt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf_xgb_base = xgb.XGBClassifier()\n",
    "#clf_xgb_base.fit(X_train, y_train)\n",
    "clf_xgb_base.fit(df_X, df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(clf_xgb_base, height=0.2)\n",
    "plt.title(\"xgboost.plot_importance(model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (cool. it worked . . . just pass the df into xgboost . . . and then the plots are perfect . . . note, need to take training/test split, etc, still . . . but later . . . this is perfect for viz now !!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = clf_xgb_base\n",
    "xgb.plot_importance(model, importance_type=\"cover\")\n",
    "plt.title('xgboost.plot_importance(model, importance_type=\"cover\")')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(model, importance_type=\"gain\")\n",
    "plt.title('xgboost.plot_importance(model, importance_type=\"gain\")')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (see. inconsistent . . . (get vocab down . . . )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Y. Explain predictions\n",
    "\n",
    "(\"Here we use the Tree SHAP implementation integrated into XGBoost to explain the entire dataset (32561 samples).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes a minute or two since we are explaining over 30 thousand samples in a model with over a thousand trees\n",
    "explainer = shap.TreeExplainer(model)\n",
    "#shap_values = explainer.shap_values(X)\n",
    "shap_values = explainer.shap_values(df_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a single prediction\n",
    "Note that we use the \"display values\" data frame so we get nice strings instead of category codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap.force_plot(explainer.expected_value, shap_values[0,:], X_display.iloc[0,:])\n",
    "shap.force_plot(explainer.expected_value, shap_values[0,:], df_X.iloc[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize many predictions\n",
    "To keep the browser happy we only visualize 1,000 individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap.force_plot(explainer.expected_value, shap_values[:1000,:], X_display.iloc[:1000,:])\n",
    "shap.force_plot(explainer.expected_value, shap_values[:1000,:], df_X.iloc[:1000,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### wow. beautful. cool !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar chart of mean importance\n",
    "This takes the average of the SHAP value magnitudes across the dataset and plots it as a simple bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap.summary_plot(shap_values, X_display, plot_type=\"bar\")\n",
    "shap.summary_plot(shap_values, df_X, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### wow . . . this may really be revealing some pitching secrets . . ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP Summary Plot\n",
    "Rather than use a typical feature importance bar chart, we use a density scatter plot of SHAP values for each feature to identify how much impact each feature has on the model output for individuals in the validation dataset. Features are sorted by the sum of the SHAP value magnitudes across all samples. It is interesting to note that the relationship feature has more total model impact than the captial gain feature, but for those samples where capital gain matters it has more impact than age. In other words, capital gain effects a few predictions by a large amount, while age effects all predictions by a smaller amount.\n",
    "Note that when the scatter points don't fit on a line they pile up to show density, and the color of each point represents the feature value of that individual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap.summary_plot(shap_values, X)\n",
    "shap.summary_plot(shap_values, df_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### wow. really cool. look at game_year . . . yeah . . . that's exactly right . . . gotta show people this . . . \n",
    "\n",
    "#### (can do this by pitch_type . . . and learn tons . . . ) (and also, divide out by year . . . and get more specific insight on other categories . . . )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP Dependence Plots\n",
    "SHAP dependence plots show the effect of a single feature across the whole dataset. They plot a feature's value vs. the SHAP value of that feature across many samples. SHAP dependence plots are similar to partial dependence plots, but account for the interaction effects present in the features, and are only defined in regions of the input space supported by data. The vertical dispersion of SHAP values at a single feature value is driven by interaction effects, and another feature is chosen for coloring to highlight possible interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for name in X_train.columns:\n",
    "#    shap.dependence_plot(name, shap_values, X, display_features=X_display)\n",
    "\n",
    "for name in df_X.columns:\n",
    "    shap.dependence_plot(name, shap_values, df_X, display_features=df_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (cool. this works . . . just think thru to see what makes most sense to look at for interactions . . . )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ok . . . and there is more advanced sh in the example notebook here (not in blg article . . . ).  Take a look as come deeper into this . . . have enough to start with . . . \n",
    "https://github.com/slundberg/shap/blob/master/notebooks/tree_explainer/Census%20income%20classification%20with%20XGBoost.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ok. cool. can tell real stories with this now . . . (and apply overallt in MA as well . . . ). Awesome. Get at this . . . use use use . . .) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (so many ways to slice things and tell deep stories . . . power !!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ok. awesome. success. did this. go to bed. morning: call, and org, and go . . . ). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get the nice viz stuff from nc days .  . . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Y. Viz single trees in boost ensemble . . . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_xgb_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_tree(model)\n",
    "# pyplot.show()\n",
    "# plot_tree(model, num_trees=4)\n",
    "# plot_tree(model, num_trees=0, rankdir='LR')\n",
    "\n",
    "# hange the layout of the graph to be left to right (easier to read) \n",
    "#      by changing the rankdir argument as LR (left-to-right) rather \n",
    "#      than the default top to bottom (UT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brownlee xgb book, around pg. 26\n",
    "\n",
    "xgb.plot_tree(clf_xgb_base) ## crash: ImportError: You must install graphviz to plot tree (at dodger stadium. get this when on web.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Y. Lime . . . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import lime ## but works from python command line . . . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### look. shap is best. do read the blg to und lime. and try it from spyder . . . or from idle . . . but no worry too much tomm on it . . . "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
